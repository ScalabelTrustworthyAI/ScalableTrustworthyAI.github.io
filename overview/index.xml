<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Overviews on Scalable Trustworthy AI</title><link>https://ScalableTrustworthyAI.github.io/overview/</link><description>Recent content in Overviews on Scalable Trustworthy AI</description><generator>Hugo</generator><language>en</language><copyright>&amp;copy; Seong Joon Oh, 2024</copyright><atom:link href="https://ScalableTrustworthyAI.github.io/overview/index.xml" rel="self" type="application/rss+xml"/><item><title/><link>https://ScalableTrustworthyAI.github.io/overview/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/overview/overview/</guid><description>&lt;p>Artificial intelligence (AI) bears hope for a positive future for humanity. For example, AI could help us fight climate change by optimising power usage. Like fire and electricity have fundamentally changed the life of mankind (&lt;a href="https://www.youtube.com/watch?v=sqd516M0Y5A">quote&lt;/a>), AI may also increase overall human productivity to balance out the ageing population and increase humanity&amp;rsquo;s overall welfare.&lt;/p>
&lt;p>The status quo is that AI is far from being perfect. One of the greatest issues is that &lt;strong>AI systems are not trustworthy yet&lt;/strong>. It is difficult to understand when and why they fail. This is especially so when the models are deployed in environments that are different from the training environment (even slightly). Even worse, naive or malicious application of AI systems causes harm to humanity by amplifying political polarisation, by treating minority groups unfairly, and by jeopardising the human liberty and free will.&lt;/p></description></item></channel></rss>