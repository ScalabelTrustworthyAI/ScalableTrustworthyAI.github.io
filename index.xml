<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Scalable Trustworthy AI</title><link>https://ScalableTrustworthyAI.github.io/</link><description>Recent content on Scalable Trustworthy AI</description><generator>Hugo</generator><language>en</language><copyright>&amp;copy; Seong Joon Oh, 2024</copyright><lastBuildDate>Thu, 15 Aug 2024 15:52:22 +0200</lastBuildDate><atom:link href="https://ScalableTrustworthyAI.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Collaborating PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/lennart/</link><pubDate>Thu, 15 Aug 2024 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/lennart/</guid><description>I am a PhD student, co-supervised by Cristòbal Curio at Reutlingen University and Seong Joon Oh at the University of Tübingen. My personal page is at lennartbramlage.com.
A broader interest in decision making under uncertainty has informed my current focus on rapid and accurate uncertainty estimation in deep neural networks. I believe that the quantification and source-attribution of their failures is indispensable wherever these models interface with real world systems, be it in autonomous driving or widely accessible large language models.</description></item><item><title>Research Assistant</title><link>https://ScalableTrustworthyAI.github.io/member/johannes/</link><pubDate>Wed, 22 May 2024 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/johannes/</guid><description>I am currently pursuing my MSc in Machine Learning at the University of Tübingen and am a research assistant at the Scalable Trustworthy AI (STAI) group. I work on explainability, more specifically on training data attribution and concept attribution. My other interests include fairness and brain-inspired AI.
I completed my BSc in Cognitive Science at the University of Tübingen in 2023. Prior to my work at STAI, I was research assistant in the Neuro-Cognitive Modeling group at the University of Tübingen and intern at FANUC Germany.</description></item><item><title>MSc student</title><link>https://ScalableTrustworthyAI.github.io/member/albert/</link><pubDate>Fri, 17 May 2024 13:06:00 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/albert/</guid><description>I post a summary of my projects and ideas at aldakata.github.io.
I am a M.Sc. Machine Learning student at the University of Tübingen, working with Elisa Nguyen and Seong Joon Oh on Annotation By-products. I am especially interested in the impact of particular training data samples on the training dynamics, with a special focus on improving data efficiency.
Previously, I was a research assistant at CVUB under Petia Radeva, where I worked on how to better model label noise on Learning with Noisy Labels see paper.</description></item><item><title>MSc Student</title><link>https://ScalableTrustworthyAI.github.io/member/anastasiia/</link><pubDate>Wed, 24 Apr 2024 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/anastasiia/</guid><description>I am pursuing MSc degree in Data Science at the University of Tübingen, supervised by Seong Joon Oh and Elisa Nguyen. My research interests revolve around the intersection of training data attribution methods applied to large language models and fairness. Moreover, I am keen on operationalizing the EU AI Act within the industry, especially explainability and fairness requirements.
I obtained my Bachelor&amp;rsquo;s degree in Political Science at Lomonosov Moscow State University and my first Master&amp;rsquo;s degree in Applied Political Analysis at Higher School of Economics in Moscow.</description></item><item><title>Studying Large Language Model Behaviors Under Realistic Knowledge Conflicts</title><link>https://ScalableTrustworthyAI.github.io/publication/evgenii2024ralm/</link><pubDate>Wed, 24 Apr 2024 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/evgenii2024ralm/</guid><description/></item><item><title>Do Deep Neural Network Solutions Form a Star Domain?</title><link>https://ScalableTrustworthyAI.github.io/publication/ankit2024star/</link><pubDate>Tue, 12 Mar 2024 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/ankit2024star/</guid><description/></item><item><title>PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/ankit/</link><pubDate>Fri, 01 Mar 2024 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/ankit/</guid><description>I am a first-year PhD Student at the Tübingen AI Center, University of Tübingen, supervised by Seong Joon Oh and co-supervised by Matthias Bethge. My personal webpage is at ankitsonthalia.com.
The simplicity bias often causes machine learning models to learn semantically meaningless, &amp;ldquo;spurious&amp;rdquo; correlations between inputs and outputs. This phenomenon hurts generalization, thus leading to often unpredictable failure cases and our inability to trust AI systems. I&amp;rsquo;m interested in understanding how to encourage ML models to learn more meaningful, causal correlations instead.</description></item><item><title>Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks</title><link>https://ScalableTrustworthyAI.github.io/publication/balint2024disentanglement/</link><pubDate>Thu, 29 Feb 2024 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/balint2024disentanglement/</guid><description/></item><item><title>Pretrained Visual Uncertainties</title><link>https://ScalableTrustworthyAI.github.io/publication/kirchhof2024pretrained/</link><pubDate>Mon, 26 Feb 2024 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/kirchhof2024pretrained/</guid><description/></item><item><title>PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/stefano/</link><pubDate>Mon, 29 Jan 2024 19:50:20 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/stefano/</guid><description>I am a PhD student in the International Max-Planck Research School for Intelligent Systems (IMPRS-IS), supervised by Christian Baumgartner and co-supervised by Seong Joon Oh at the University of Tübingen.
I work on few-shot learning and domain generalisation for medical image analysis. I am passionate about developing more “human-like” machine learning methods, which mimic our ability to make predictions based on a limited number of training examples and to generalise to unsee tasks.</description></item><item><title>Exploring Practitioner Perspectives On Training Data Attribution Explanations</title><link>https://ScalableTrustworthyAI.github.io/publication/elisa2023neuripsxaiw/</link><pubDate>Wed, 01 Nov 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/elisa2023neuripsxaiw/</guid><description/></item><item><title>Trustworthy Machine Learning</title><link>https://ScalableTrustworthyAI.github.io/publication/balint2023tml/</link><pubDate>Sun, 01 Oct 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/balint2023tml/</guid><description/></item><item><title>A Bayesian Perspective On Training Data Attribution</title><link>https://ScalableTrustworthyAI.github.io/publication/elisa2023neurips/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/elisa2023neurips/</guid><description/></item><item><title>ID and OOD Performance Are Sometimes Inversely Correlated on Real-world Datasets</title><link>https://ScalableTrustworthyAI.github.io/publication/teney2023neurips/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/teney2023neurips/</guid><description/></item><item><title>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates</title><link>https://ScalableTrustworthyAI.github.io/publication/kirchhof2023neuripsdb/</link><pubDate>Fri, 01 Sep 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/kirchhof2023neuripsdb/</guid><description/></item><item><title>Neglected Free Lunch -- Learning Image Classifiers Using Annotation Byproducts</title><link>https://ScalableTrustworthyAI.github.io/publication/han2023iccv/</link><pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/han2023iccv/</guid><description/></item><item><title>Scratching Visual Transformer's Back with Uniform Attention</title><link>https://ScalableTrustworthyAI.github.io/publication/nam2023iccv/</link><pubDate>Tue, 25 Jul 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/nam2023iccv/</guid><description/></item><item><title>Arnas Uselis | PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/arnas/</link><pubDate>Mon, 24 Jul 2023 11:26:00 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/arnas/</guid><description>I&amp;rsquo;m a PhD student at the University of Tübingen and the International Max Planck Research School for Intelligent Systems (IMPRS-IS), under the supervision of Seong Joon Oh.
I&amp;rsquo;m primarily interested in enhancing the generalization capabilities of current machine learning systems. This involves learning robust representations from raw data and using them in a way that accurately reflects data generative processes. I believe that mimicking the structure of the real-world in our models, both in how we learn representations and in how we use them, is a key part of making progress towards trustworthy systems.</description></item><item><title>Probabilistic Contrastive Learning Recovers the Correct Aleatoric Uncertainty of Ambiguous Inputs</title><link>https://ScalableTrustworthyAI.github.io/publication/kirchhof2023icml/</link><pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/kirchhof2023icml/</guid><description/></item><item><title>URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates</title><link>https://ScalableTrustworthyAI.github.io/publication/kirchhof2023uaieai/</link><pubDate>Mon, 24 Jul 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/kirchhof2023uaieai/</guid><description/></item><item><title>MSc student</title><link>https://ScalableTrustworthyAI.github.io/member/evgenii/</link><pubDate>Sat, 17 Jun 2023 13:06:00 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/evgenii/</guid><description>My personal page is at kortukov.github.io.
I am currently pursuing my MSc degree in Machine Learning at the University of Tübingen, supervised by Seong Joon Oh, Elisa Nguyen and Alexander Rubinstein. My research interests primarily revolve around mitigating the risks associated with increasingly powerful ML systems. My current focus involves exploring data-driven explainability methods and their application to language models.
I receieved my Bachelor degree in Computer Science from Lomonosov Moscow State University in 2020.</description></item><item><title>MSc Student</title><link>https://ScalableTrustworthyAI.github.io/member/balint/</link><pubDate>Fri, 02 Jun 2023 11:26:00 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/balint/</guid><description>I am a Machine Learning MSc student at the University of Tübingen, supervised by Seong Joon Oh, Michael Kirchhof, and Alexander Rubinstein.
I am interested in model architectures capable of representing different sources of uncertainty. My goal is to contribute to the theoretical foundations of uncertainty in machine learning while developing scalable practical solutions.
I received my BSc degree in Computer Science from ELTE Eötvös Loránd University in 2021 with the Outstanding Student of the Faculty award.</description></item><item><title>Playing repeated games with Large Language Models</title><link>https://ScalableTrustworthyAI.github.io/publication/elif2023arxiv/</link><pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/elif2023arxiv/</guid><description/></item><item><title>Collaborating PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/michael/</link><pubDate>Fri, 31 Mar 2023 11:26:00 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/michael/</guid><description>I&amp;rsquo;m a PhD student in the International Max-Planck Research School for Intelligent Systems (IMPRS-IS), co-supervised by Enkelejda Kasneci and Seong Joon Oh at the University of Tübingen.
My goal is making machine learning more trustworthy by delivering pretrained uncertainty estimates along with each prediction. To this end, I&amp;rsquo;m developing probabilistic embeddings that represent a model&amp;rsquo;s uncertainty directly in its embedding space. I love to understand and prove things first from a theoretical perspective first (like MCInfoNCE) and then scale them to large datasets as in the new URL benchmark.</description></item><item><title>Postdoc Opportunity: Scalable Trustworthy AI - Novel Dataset Development</title><link>https://ScalableTrustworthyAI.github.io/opening/postdoc-mar2023/</link><pubDate>Thu, 16 Mar 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/opening/postdoc-mar2023/</guid><description/></item><item><title>Master Projects</title><link>https://ScalableTrustworthyAI.github.io/opening/thesis/</link><pubDate>Thu, 23 Feb 2023 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/opening/thesis/</guid><description/></item><item><title>PhD Students</title><link>https://ScalableTrustworthyAI.github.io/opening/phd/</link><pubDate>Mon, 05 Sep 2022 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/opening/phd/</guid><description/></item><item><title>ECCV Caption: Correcting False Negatives by Collecting Machine-and-Human-verified Image-Caption Associations for MS-COCO</title><link>https://ScalableTrustworthyAI.github.io/publication/chun2022eccv/</link><pubDate>Mon, 01 Aug 2022 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/chun2022eccv/</guid><description/></item><item><title>Dataset Condensation via Efficient Synthetic-Data Parameterization</title><link>https://ScalableTrustworthyAI.github.io/publication/kim2022icml/</link><pubDate>Fri, 01 Jul 2022 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/kim2022icml/</guid><description/></item><item><title>Weakly Supervised Semantic Segmentation Using Out-of-Distribution Data</title><link>https://ScalableTrustworthyAI.github.io/publication/lee2022cvpr/</link><pubDate>Wed, 01 Jun 2022 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/lee2022cvpr/</guid><description/></item><item><title>Group Leader</title><link>https://ScalableTrustworthyAI.github.io/member/joon/</link><pubDate>Tue, 24 May 2022 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/joon/</guid><description>My personal page is at seongjoonoh.com.
I am an independent group leader at the Tübingen AI Center at the University of Tübingen, where I lead the group on Scalable Trustworthy AI (STAI). I am interested in training reliable models (e.g. explainable, robust, and probabilistic models) and obtaining the necessary human supervision and guidance in a cost-effective way.
I have been a research scientist at NAVER AI Lab for 3.5 years. I received my PhD in computer vision and machine learning at Max-Planck Institute for Informatics in 2018, under the supervision of Bernt Schiele and Mario Fritz.</description></item><item><title>PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/alex/</link><pubDate>Tue, 24 May 2022 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/alex/</guid><description>I am a PhD Student at the Tübingen AI Center at the University of Tübingen and the International Max Planck Research School for Intelligent Systems (IMPRS-IS), where I work in the group on Scalable Trustworthy AI (STAI). I am interested in researching the ways of making models not only perfectly classify MNIST but also not fail too much on real world tasks. In case possibilities of some models are limited to academic datasets, I want to understand why that is the case.</description></item><item><title>PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/elif/</link><pubDate>Tue, 24 May 2022 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/elif/</guid><description>I am a first year PhD student in the Scalable Trustworthy AI (STAI) group at the Tübingen AI Center.
I received my Master’s degree in Computer Science from University of Tübingen in 2022, and Bachelor’s in Computer Science from Saarland University in 2019. I am interested in understanding abstractions humans and neural networks perform in visual representations and how to use these to improve model robustness and Human-Computer Interaction.</description></item><item><title>Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective</title><link>https://ScalableTrustworthyAI.github.io/publication/scimeca2022iclr/</link><pubDate>Fri, 01 Apr 2022 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/publication/scimeca2022iclr/</guid><description/></item><item><title>PhD Student</title><link>https://ScalableTrustworthyAI.github.io/member/elisa/</link><pubDate>Tue, 12 Jul 2016 15:52:22 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/member/elisa/</guid><description>My personal page is at elisanguyen.github.io.
I am a PhD student at the University of Tübingen and the International Max Planck Research School for Intelligent Systems (IMPRS-IS). My research focus is on exploring training data attribution as a means of helping humans understand model behavior.
I absolved my Master’s in Interaction Technology at the University of Twente and my Bachelor’s in Computer Science and Management at the Nordakademie. I did my Bachelor studies in a dual study format in cooperation with Airbus, where I also worked for about 2.</description></item><item><title>Markdown ipsum</title><link>https://ScalableTrustworthyAI.github.io/post/post1/</link><pubDate>Tue, 12 Jul 2016 15:50:58 +0200</pubDate><guid>https://ScalableTrustworthyAI.github.io/post/post1/</guid><description>Magorum notissima limite sua pars simus sumptis Incessit ignota coniunx serpunt Lorem markdownum ab cunas, semine commissus matrona manibusque plumis, nova sub Spartana loca ignibus. In partibus muneris, paludes rara, plectrumque, fontis, concubitus a locoque demptos exclamat conde ab aethera nihil.
Quamquam abit vulnere nec intus decidit clamor Nocuit quae tamen timent aperta Cervice preces totumque postquam nunc iacit sive Potestas te sis putaret sceleri totiens Retro profundo ad sede Iovem in este In ait flumina intrare Troiae Ut fatis praecordia alvum iamque, adeo tympana male silvas.</description></item><item><title/><link>https://ScalableTrustworthyAI.github.io/overview/overview/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/overview/overview/</guid><description>Artificial intelligence (AI) bears hope for a positive future for humanity. For example, AI could help us fight climate change by optimising power usage. Like fire and electricity have fundamentally changed the life of mankind (quote), AI may also increase overall human productivity to balance out the ageing population and increase humanity&amp;rsquo;s overall welfare.
The status quo is that AI is far from being perfect. One of the greatest issues is that AI systems are not trustworthy yet.</description></item><item><title>Trustworthy Machine Learning</title><link>https://ScalableTrustworthyAI.github.io/courses/tml_winter_2223/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/courses/tml_winter_2223/</guid><description>Overview Goal Students will be able to critically read, assess, and discuss research work in Trustworthy Machine Learning (TML). Students will gain the technical background to implement basic TML techniques in a deep learning framework. Students will be ready to conduct their own research in TML and make contributions to the research community. Prerequisites Familiarity with Python and PyTorch coding. A pass grade from the Deep Learning Course (or equivalent). Basic knowledge of machine learning concepts.</description></item><item><title>Trustworthy Machine Learning</title><link>https://ScalableTrustworthyAI.github.io/courses/tml_winter_2324/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/courses/tml_winter_2324/</guid><description>Overview Goal Students will be able to critically read, assess, and discuss research work in Trustworthy Machine Learning (TML). Students will gain the technical background to implement basic TML techniques in a deep learning framework. Students will be ready to conduct their own research in TML and make contributions to the research community. Prerequisites Familiarity with Python and PyTorch coding. A pass grade from the Deep Learning Course (or equivalent). Basic knowledge of machine learning concepts.</description></item><item><title>Trustworthy Machine Learning</title><link>https://ScalableTrustworthyAI.github.io/courses/tml_winter_2425/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://ScalableTrustworthyAI.github.io/courses/tml_winter_2425/</guid><description>Overview Goal Students will be able to critically read, assess, and discuss research work in Trustworthy Machine Learning (TML). Students will gain the technical background to implement basic TML techniques in a deep learning framework. Students will be ready to conduct their own research in TML and make contributions to the research community. Prerequisites Familiarity with Python and PyTorch coding. A pass grade from the Deep Learning Course (or equivalent). Basic knowledge of machine learning concepts.</description></item></channel></rss>