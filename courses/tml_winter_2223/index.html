<!doctype html><html lang=en-us><head><script async src="https://www.googletagmanager.com/gtag/js?id=G-RF7RQJFKWW"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-RF7RQJFKWW")</script><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=theme content="hugo-academic-group"><script src=https://ajax.googleapis.com/ajax/libs/jquery/1.12.4/jquery.min.js></script>
<script src=https://ScalableTrustworthyAI.github.io/js/hugo-academic-group.js></script>
<link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/bootstrap.min.css><script src=https://ScalableTrustworthyAI.github.io/js/bootstrap.min.js></script>
<link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.6.0/styles/default.min.css><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/github-fork-ribbon-css/0.2.2/gh-fork-ribbon.min.css><script src=https://ScalableTrustworthyAI.github.io/js/highlight.pack.js></script>
<script>hljs.initHighlightingOnLoad()</script><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/font-awesome.min.css><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/academicons.min.css><link rel=stylesheet href="//fonts.googleapis.com/css?family=Lato:100,300,400,700|Merriweather:100,400,700|Roboto+Mono"><link rel=stylesheet href=https://ScalableTrustworthyAI.github.io/css/hugo-academic-group.css><link rel="shortcut icon" href=https://ScalableTrustworthyAI.github.io/img/favicon.png type=image/x-icon><link rel=canonical href=https://ScalableTrustworthyAI.github.io/courses/tml_winter_2223/><title>Trustworthy Machine Learning | Scalable Trustworthy AI</title></head><body><div class=home-anchor id=home></div><nav class="navbar navbar-default navbar-fixed-top" id=navbar-main><div class=container><div class=navbar-header><button type=button class="navbar-toggle collapsed" data-toggle=collapse data-target=.navbar-collapse aria-expanded=false>
<span class=sr-only>Toggle navigation</span>
<span class=icon-bar></span>
<span class=icon-bar></span>
<span class=icon-bar></span></button><div class=navbar-brand><a class=logo href=https://ScalableTrustworthyAI.github.io/><img src=https://ScalableTrustworthyAI.github.io/img/stai_logo.png alt="Research group logo"></img></a></div></div><div class="collapse navbar-collapse" id=#navbar-collapse-1><ul class="nav navbar-nav navbar-right"><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#top>Home</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#overview>Overview</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#members>Members</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#publications>Publications</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#courses>Courses</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#openings>Openings</a></li><li class=nav-item><a data-scroll href=https://ScalableTrustworthyAI.github.io/#contact>Contact</a></li></ul></div></div></nav><div><br></div><div class=container><div class=row itemprop=author itemscope itemtype=http://schema.org/Person><div class="visible-sm visible-xs"></div><div class=col-sm-12 itemprop=description><h2>Trustworthy Machine Learning</h2><h4>Winter Semester 2022-2023, University of TÃ¼bingen</h4></div><div class=col-sm-8 itemprop=description><div class=article-style>As machine learning technology gets applied to actual products and solutions, new challenges have emerged. Models unexpectedly fail to generalise well to small changes in the distribution; some models are found to utilise sensitive features that could treat certain demographic user groups unfairly; models tend to be confident on novel types of data; models cannot communicate the rationale behind their decisions effectively with the end users like medical staff to maximise the human-machine synergies. Collectively, we face a trustworthiness issue with the current machine learning technology. A large fraction of the machine learning research nowadays is dedicated to expanding the frontier of Trustworthy Machine Learning (TML). The course covers a theoretical and technical background for key topics in TML. We conduct a critical review of important classical and contemporary research papers on related topics and provide hands-on practicals to implement TML techniques.</div></div><div class=col-sm-4 itemprop=description><img src=https://ScalableTrustworthyAI.github.io/img/DALLE_TML.png class=course-banner itemprop=image></div></div><div class=row itemprop=author itemscope itemtype=http://schema.org/Person><div class=col-sm-12 itemprop=description><div class=course-article-style><h2 id=overview>Overview</h2><h3 id=goal>Goal</h3><ol><li>Students will be able to critically read, assess, and discuss research work in Trustworthy Machine Learning (TML).</li><li>Students will gain the technical background to implement basic TML techniques in a deep learning framework.</li><li>Students will be ready to conduct their own research in TML and make contributions to the research community.</li></ol><h3 id=prerequisites>Prerequisites</h3><ul><li>Familiarity with Python and PyTorch coding.</li><li>A pass grade from the Deep Learning Course (or equivalent).</li><li>Basic knowledge of machine learning concepts.</li><li>Basic maths: multivariate calculus, linear algebra, probability, statistics, and optimisation.</li></ul><h3 id=reading-list>Reading list</h3><p>Recommended papers from each topic.</p><details><summary>&#9656; OOD Generalisation (click to expand)</summary><ul><li><a href=https://arxiv.org/pdf/1904.05046.pdf>Generalizing from a Few Examples: A Survey on Few-Shot Learning</a></li><li><a href=https://changliu00.github.io/dg_survey/dg_survey.pdf>Generalizing to Unseen Domains: A Survey on Domain Generalization</a></li><li><a href=https://arxiv.org/pdf/2007.01434.pdf>In Search of Lost Domain Generalization</a></li><li><a href=https://arxiv.org/pdf/2011.03395.pdf>Underspecification Presents Challenges for Credibility in Modern Machine Learning</a></li><li><a href=https://arxiv.org/pdf/2004.07780.pdf>Shortcut Learning in Deep Neural Networks</a></li><li><a href=https://coallaoh.github.io/data/scimeca2022iclr.pdf>Which Shortcut Cues Will DNNs Choose? A Study from the Parameter-Space Perspective</a></li><li><a href=http://proceedings.mlr.press/v139/creager21a/creager21a.pdf>Environment Inference for Invariant Learning</a></li><li><a href=https://arxiv.org/abs/1811.12231>ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness.</a></li></ul></details><details><summary>&#9656; Explainability (click to expand)</summary><ul><li><a href="https://www.sciencedirect.com/science/article/pii/S0004370218305988?casa_token=HfkXcrQBLj4AAAAA:i4QhGuIsXDlmZ_EhfMyCCFkn2tnoI0JsVM5O8Qggf01HG0aMZeatCEY2UsJxKfvYD2BtwG6EZkI">Explanation in artificial intelligence: Insights from the social sciences</a></li><li><a href=https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf>Sanity Checks for Saliency Maps</a></li><li><a href=https://proceedings.neurips.cc/paper/2019/file/fe4b8556000d0f0cae99daa5c5c5a410-Paper.pdf>A benchmark for interpretability methods in deep neural networks</a></li><li><a href="https://dl.acm.org/doi/abs/10.1145/3411764.3445188?casa_token=6ZTaKuOCMUYAAAAA:vhJnNcizK2MMk4huftyKY4ANzL0_v2g4_eOc2DnZ582koWoD4ohfagwS4np45fNWLOeTeODd-BeL">Expanding Explainability: Towards Social Transparency in AI systems</a></li><li><a href=http://link.springer.com/10.1007/978-3-030-60117-1_33>Human-Centered Explainable AI: Towards a Reflective Sociotechnical Approach</a></li><li><a href=https://arxiv.org/abs/1312.6034>Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps</a></li><li><a href=http://proceedings.mlr.press/v70/sundararajan17a.html>Axiomatic attribution for deep networks</a></li><li><a href=https://arxiv.org/pdf/1706.03825.pdf>SmoothGrad: removing noise by adding noise</a></li><li><a href=https://proceedings.neurips.cc/paper/2017/hash/8a20a8621978632d76c43dfd28b67767-Abstract.html>A Unified Approach to Interpreting Model Predictions</a></li><li><a href=http://proceedings.mlr.press/v80/kim18d/kim18d.pdf>Interpretability Beyond Feature Attribution: Quantitative Testing with Concept Activation Vectors (TCAV)</a></li><li><a href="https://openreview.net/pdf?id=BJ5UeU9xx">Visualizing Deep Neural Network Decisions: Prediction Difference Analysis</a></li><li><a href=http://cnnlocalization.csail.mit.edu/Zhou_Learning_Deep_Features_CVPR_2016_paper.pdf>Learning Deep Features for Discriminative Localization</a></li><li><a href=https://arxiv.org/abs/1610.02391>Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization</a></li><li><a href=https://dl.acm.org/doi/abs/10.1145/2939672.2939778>&ldquo;Why Should I Trust You?&rdquo;: Explaining the Predictions of Any Classifier</a></li><li><a href=10.23915/distill.00007>Feature Visualization</a></li><li><a href=http://arxiv.org/abs/1703.04730>Understanding Black-box Predictions via Influence Functions</a></li><li><a href=http://arxiv.org/abs/2002.08484>Estimating Training Data Influence by Tracing Gradient Descent</a></li></ul></details><details><summary>&#9656; Uncertainty (click to expand)</summary><ul><li><a href="https://openreview.net/forum?id=Hkg4TI9xl">A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks</a></li><li><a href=http://proceedings.mlr.press/v70/guo17a/guo17a.pdf>On calibration of modern neural networks</a></li><li><a href="https://openreview.net/forum?id=HyxCxhRcY7">Deep Anomaly Detection with Outlier Exposure</a></li><li><a href=https://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf>A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks</a></li><li><a href=http://proceedings.mlr.press/v48/gal16.pdf>Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning</a></li><li><a href=http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles.pdf>Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles</a></li><li><a href=https://papers.nips.cc/paper/2018/hash/be3087e74e9100d4bc4c6268cdbe8456-Abstract.html>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</a></li><li><a href=https://arxiv.org/abs/1803.05407>Averaging weights leads to wider optima and better generalization</a></li><li><a href=http://proceedings.mlr.press/v119/dusenberry20a/dusenberry20a.pdf>Efficient and scalable Bayesian neural nets with rank-1 factors</a></li><li><a href=https://papers.nips.cc/paper/7141-what-uncertainties-do-we-need-in-bayesian-deep-learning-for-computer-vision>What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?</a></li><li><a href=https://papers.nips.cc/paper/8556-addressing-failure-prediction-by-learning-model-confidence.pdf>Addressing Failure Prediction by Learning Model Confidence</a></li><li><a href=https://arxiv.org/abs/1511.06314>Why M Heads are Better than One: Training a Diverse Ensemble of Deep Networks</a></li><li><a href=https://openaccess.thecvf.com/content_cvpr_2018/papers/Firman_DiverseNet_When_One_CVPR_2018_paper.pdf>DiverseNet: When One Right Answer is not Enough</a></li></ul></details><details><summary>&#9656; Evaluation (click to expand)</summary><ul><li><a href=https://aclanthology.org/2021.eacl-main.86.pdf>Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets</a></li><li><a href=https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123700681.pdf>A Metric Learning Reality Check</a></li><li><a href=https://openaccess.thecvf.com/content_CVPR_2020/papers/Choe_Evaluating_Weakly_Supervised_Object_Localization_Methods_Right_CVPR_2020_paper.pdf>Evaluating Weakly-Supervised Object Localization Methods Right</a></li><li><a href=http://proceedings.mlr.press/v97/recht19a/recht19a.pdf>Do ImageNet Classifiers Generalize to ImageNet?</a></li><li><a href=http://proceedings.mlr.press/v97/locatello19a/locatello19a.pdf>Challenging Common Assumptions in the Unsupervised Learning of Disentangled Representations.</a></li><li><a href=https://papers.nips.cc/paper/2018/file/c1fea270c48e8079d8ddf7d06d26ab52-Paper.pdf>Realistic Evaluation of Deep Semi-Supervised Learning Algorithms</a></li><li><a href=https://openaccess.thecvf.com/content_cvpr_2017/papers/Xian_Zero-Shot_Learning_-_CVPR_2017_paper.pdf>Zero-Shot Learning-The Good, the Bad and the Ugly</a></li><li><a href=https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf>Sanity Checks for Saliency Maps</a></li><li><a href=https://arxiv.org/pdf/2007.01434.pdf>In Search of Lost Domain Generalization</a></li><li><a href="https://openreview.net/forum?id=mPducS1MsEK">Are We Learning Yet? A Meta-Review of Evaluation Failures Across Machine Learning</a></li><li><a href=https://arxiv.org/abs/2108.02497>How to avoid machine learning pitfalls: a guide for academic researchers</a></li><li><a href=http://aclanthology.lst.uni-saarland.de/D19-1224.pdf>Show Your Work: Improved Reporting of Experimental Results</a></li></ul></details><h2 id=policies>Policies</h2><details><summary>&#9656; Registration & Exercise 0 (click to expand)</summary><p>The <strong>registration period</strong> ends on <strong>27 October 2022</strong>. During this period, you are required to work on Exercise 0 and submit it.
The Exercise 0 is an individual exercise and will be based on the prerequisite materials for the course.
It will be published right after the first lecture on 21 October 2022 on this web page (see <code>Schedule & exercises</code> section below).
Students who wish to enrol must submit their work on Exercise 0.
To prevent an empty submission, there is a minimal score (30%) for the exercise.
The necessary prerequisite materials will be lectured in the first lecture.</p><p>Exercise 0 serves two purposes:</p><ul><li>For us to admit students who are sufficiently motivated for the course.</li><li>For you to gauge your own readiness for the course.</li></ul><p>The timeline for registration is as follows:</p><ul><li>Now-20 October 2022: Check time & location for the course.</li><li>21 October 2022: Attend the first lecture.</li><li>21-27 October 2022: Work on Exercise 0 & submit.</li><li>28 October 2022: Register students who have submitted Exercise 0.</li></ul></details><h3 id=grading-policy>Grading policy</h3><p><img src=https://ScalableTrustworthyAI.github.io/img/grading_scheme.png alt=TML_grading></p><p>The final grade is essentially based on your exam performance. However, to encourage the participation in the exercises,
we award bonus boosts on the final grade based on the exercise performance.</p><p><strong>Exercise 0 and registration for course</strong></p><ul><li>We admit only those who submit the zeroth exercise to the course.</li><li>Exercise 0 is an individual exercise.</li></ul><p><strong>Exercise 1-3 score E% (0 - 100%)</strong></p><ul><li>Average score from Exercises 1-3.</li><li>Exercises 1-3 are group exercises.</li></ul><p><strong>Admission to exam</strong></p><ul><li>When the exercise grade is $\geq 60$%, you are granted the right to participate in the exam.</li><li>Can take care of exceptional cases on an individual basis.</li></ul><p><strong>Exam score (0 - 100%)</strong></p><ul><li>Based on your performance on your exam.</li><li>The pass threshold for the course will be $\geq 50$% on the exam.</li><li>There will be two exams: Exam 1 and Exam 2.<ul><li>Case A: Sit Exam 1 and pass â Exam 1 score will be final (cannot sit Exam 2).</li><li>Case B: Sit Exam 1 and fail â You may sit Exam 2; Exam 2 score will be final.</li><li>Case C: Choose to skip Exam 2 â Exam 2 score will be final.</li></ul></li></ul><p><strong>Final score (0 - 100%)</strong></p><ul><li>The final score is your exam score with possible increments from your exercise performance.</li><li>Increment from exercise = $(E-60)/40 \times 20$%, where $E$% is your exercise score.<ul><li>If $E=100$, you will get 20%p additional points on top of your exam grade.</li><li>If $E=60$, no additional points will be awarded.</li><li>If $E&lt; 60$, you will not be admitted to the exam.</li></ul></li><li>Increments from exercise will not let you pass the course if your exam score is below $50%$.</li></ul><p><strong>Final grade (1,0 - 5,0)</strong></p><ul><li>The final grade of the course will be based on the final score.</li></ul><h2 id=communication>Communication</h2><h3 id=lecturer>Lecturer</h3><ul><li><a href=../../member/joon/>Seong Joon Oh</a></li></ul><h3 id=tutors>Tutors</h3><ul><li><a href=../../member/alex/>Alexander Rubinstein</a></li><li><a href=../../member/elif/>Elif Akata</a></li><li><a href=../../member/elisa/>Elisa Nguyen</a></li><li><a href=https://www.eml-unitue.de/people/michael-kirchhof>Michael Kirchhof</a></li></ul><h3 id=central-email>Central email</h3><p>Please use the STAI group email <code>stai.there@gmail.com</code> to</p><ul><li>Submit your Colab exercises;</li><li>Ask questions; and</li><li>Send us feedback.</li></ul><h3 id=discord-forum>Discord forum</h3><p>For those who&rsquo;re registered for the course, ask the lecturer or tutors to add you to the Discord channel. We need your name and email address. Use it for</p><ul><li>Asking questions;</li><li>Sending us feedback;</li><li>Receiving official announcements; and</li><li>Communicating others (e.g. finding partners).</li></ul><h2 id=when--where>When & where</h2><h3 id=lecture--tutorial-location>Lecture & tutorial location</h3><p>Seminarraum C118a (Informatik Sand 14)
<a href=https://goo.gl/maps/mP6Q92s6QcLoHK5v7>https://goo.gl/maps/mP6Q92s6QcLoHK5v7</a></p><h3 id=lecture-times>Lecture times</h3><ul><li>Fridays</li><li>1st session: 14:15-15:00</li><li>5-min break: 15:00-15:05</li><li>2nd session: 15:05-15:50</li></ul><h3 id=tutorial-times>Tutorial times</h3><ul><li>Fridays 16:00-18:00</li><li>Up to discussion</li></ul><h3 id=exam-dates>Exam dates</h3><ul><li>Exam 1: 21/02/2023</li><li>Exam 2: 11/04/2023</li></ul><h2 id=schedule--exercises>Schedule & exercises</h2><table class="table table-striped table-bordered"><thead><tr><th>#</th><th>Date</th><th>Content</th><th>Exercises</th><th>Lead tutor</th></tr></thead><tbody><tr><td>L1</td><td>21/10/2022</td><td><a href="https://docs.google.com/presentation/d/1HbM1oXIwduT8Lldj5S1DlOO_cZC8P2_OK9Chh4R2IZo/edit?usp=sharing">Introduction</a></td><td><a href="https://colab.research.google.com/drive/1m34hoUQApiuT673oKcR97utOXFdmYRRm?usp=sharing">Exercise 0</a> (Due: 27/10/2022)</td><td>Elif</td></tr><tr><td>L2</td><td>28/10/2022</td><td><a href="https://docs.google.com/presentation/d/1HQtkLVuxCjmU0mGEebIOOnHZED5lmBcrf1vPBhIzHCA/edit?usp=sharing">OOD Generalisation - Definition and limitations</a> (<a href="https://youtu.be/DqrWiaEVKx4&list=PL5iDtgKiCe5SuhyyGQT5OImWmsItHr8OQ">Video</a>)</td><td><a href="https://colab.research.google.com/drive/1RyRuqBCoXb3zRZ2lxoS4vt1tvDHpSNHx?usp=sharing">Exercise 1</a> (<a href="https://www.youtube.com/watch?v=nIGyYBKPfjk">Tutorial video</a>)<br>(Due: 21/11/2022)</td><td>Alex</td></tr><tr><td>L3</td><td>04/11/2022</td><td><a href="https://docs.google.com/presentation/d/1NODSaP31IuT0923X6PQ0AkwqgEcyxuZZYpFf9kkdQQw/edit?usp=sharing">OOD Generalisation - Cue selection problem</a> (<a href="https://youtu.be/5vWcflV8Gq8&list=PL5iDtgKiCe5SuhyyGQT5OImWmsItHr8OQ">Video</a>)</td><td></td><td>Alex</td></tr><tr><td>L4</td><td>11/11/2022</td><td><a href="https://docs.google.com/presentation/d/1G459l_R_ir_mxuIJtaMHk-RLzXaEumlwB-oG0AeDfsk/edit?usp=sharing">OOD Generalisation - Cue selection and adversarial generalisation</a> (<a href="https://youtu.be/AZZv105ubbk&list=PL5iDtgKiCe5SuhyyGQT5OImWmsItHr8OQ">Video</a>)</td><td></td><td>Alex</td></tr><tr><td>L5</td><td>18/11/2022</td><td><a href="https://docs.google.com/presentation/d/1a9OGP5zZjEdOY5fb0L3CGskMjlS65U5AFYPV9XbfP-I/edit?usp=sharing">More OOD and Explainability - Definition and limitations</a> (<a href="https://youtu.be/TPVwXbXH0l8&list=PL5iDtgKiCe5SuhyyGQT5OImWmsItHr8OQ">Video</a>)</td><td></td><td>Alex / Elisa</td></tr><tr><td>L6</td><td>25/11/2022</td><td><a href="https://docs.google.com/presentation/d/1GZBCZQWzqjAsUnNfgUmO9Ge1DX0M5flEjWMfHtabsJU/edit?usp=sharing">Explainability - Attribution</a> (<a href="https://youtu.be/Vk6UERA_HuI&list=PL5iDtgKiCe5SuhyyGQT5OImWmsItHr8OQ">Video</a>)</td><td><a href="https://colab.research.google.com/drive/1UO4hWoUKWB9lO_dACNDf8fxeBoXr4cvd?usp=sharing">Exercise 2</a> (Due: 16/12/2022)</td><td>Elisa</td></tr><tr><td>L7</td><td>02/12/2022</td><td><a href="https://docs.google.com/presentation/d/1ZXX_x8EJjUooktGP26fxntsa8mcFb0sKTALQZYVhJJY/edit?usp=sharing">Explainability - Attribution & evaluation</a> (<a href="https://youtu.be/nN_PjSWAxdQ&list=PL5iDtgKiCe5SuhyyGQT5OImWmsItHr8OQ">Video</a>)</td><td></td><td>Elisa</td></tr><tr><td>L8</td><td>09/12/2022</td><td><a href="https://docs.google.com/presentation/d/1iRvMVG6xOf2bIm2-zE05XMFpY6LVoaEL7gYSDejQpu4/edit?usp=sharing">Explainability - Model explanation</a> (<a href=https://youtu.be/ZqFRKoupo_4>Video</a>)</td><td></td><td>Elisa</td></tr><tr><td></td><td></td><td>Christmas Break</td><td></td><td></td></tr><tr><td>L9</td><td>13/01/2023</td><td>Uncertainty - Definition and evaluation</td><td>Exercise 3 (Due: 30/01/2023)</td><td>Michael</td></tr><tr><td>L10</td><td>20/01/2023</td><td>Uncertainty - Epistemic uncertainty</td><td></td><td>Michael</td></tr><tr><td>L11</td><td>27/01/2023</td><td>Uncertainty - Aleatoric uncertainty</td><td></td><td>Michael</td></tr><tr><td>L12</td><td>03/02/2023</td><td>Topics on evaluation</td><td></td><td>Elif</td></tr></tbody></table></div></div></div></div><footer class=site-footer><div class=container><p class=powered-by>Â© Seong Joon Oh, 2022 &#183;
Partially powered by the <a href=https://github.com/gcushen/hugo-academic target=_blank>Academic theme</a> for <a href=http://gohugo.io target=_blank>Hugo</a>.
<span class=pull-right><a href=#home id=back_to_top><span class=button_icon><i class="fa fa-chevron-up fa-2x" aria-hidden=true></i></span></a></span></p></div></footer><script src=//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.4/TweenMax.min.js></script>
<script src=//cdnjs.cloudflare.com/ajax/libs/gsap/latest/plugins/ScrollToPlugin.min.js></script>
<script type=text/x-mathjax-config>
 MathJax.Hub.Config({ tex2jax: { inlineMath: [['$','$'], ['\\(','\\)']] } });
</script><script type=text/javascript async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script></body></html>